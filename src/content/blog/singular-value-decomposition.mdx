---
title: "Singular Value Decomposition: The Anatomy of a Linear Map"
date: "2026-02-07"
category: "Linear Algebra"
excerpt: "A thorough exposition of the SVD — the most important matrix factorization in applied mathematics — revealing the geometric action of any linear transformation as a composition of rotations and scalings."
tags:
  - "linear-algebra"
  - "matrix-decomposition"
  - "numerical-methods"
---

## Introduction

The **singular value decomposition** (SVD) is, without exaggeration, the most useful factorization in linear algebra. Every matrix — rectangular, singular, rank-deficient — has an SVD. It reveals the geometric essence of a linear transformation: any linear map is a rotation, followed by a scaling along coordinate axes, followed by another rotation. The SVD is indispensable in data science, signal processing, statistics, and numerical analysis.

## The Theorem

**Theorem (SVD).** Let $A$ be an $m \times n$ real matrix of rank $r$. Then there exist:
- An $m \times m$ orthogonal matrix $U$
- An $n \times n$ orthogonal matrix $V$
- An $m \times n$ "diagonal" matrix $\Sigma$ with entries $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$

such that:

$$
A = U \Sigma V^T
$$

The values $\sigma_1, \ldots, \sigma_r$ are the **singular values** of $A$. The columns of $U$ are the **left singular vectors**, and the columns of $V$ are the **right singular vectors**.

### Compact Form

Writing only the nonzero part:

$$
A = \sum_{i=1}^r \sigma_i \mathbf{u}_i \mathbf{v}_i^T
$$

Each term $\sigma_i \mathbf{u}_i \mathbf{v}_i^T$ is a rank-1 matrix. The SVD expresses $A$ as a sum of rank-1 matrices, ordered by importance.

## Geometric Interpretation

The SVD reveals that any linear map $A: \mathbb{R}^n \to \mathbb{R}^m$ acts as:

1. **Rotate/reflect** in the domain by $V^T$
2. **Scale** along coordinate axes by $\sigma_1, \ldots, \sigma_r$ (sending the unit sphere to an ellipsoid)
3. **Rotate/reflect** in the codomain by $U$

The singular values are the **semi-axes of the image ellipsoid**: $A$ maps the unit sphere $\{x : \|x\| = 1\}$ to an ellipsoid with semi-axes $\sigma_1 \geq \cdots \geq \sigma_r$.

## Relation to Eigenvalues

The singular values are the square roots of the eigenvalues of $A^TA$ (or $AA^T$):

$$
\sigma_i = \sqrt{\lambda_i(A^T A)}
$$

The right singular vectors $v_i$ are eigenvectors of $A^TA$; the left singular vectors $u_i$ are eigenvectors of $AA^T$.

**Key difference:** Eigenvalues exist only for square matrices and may be complex. Singular values exist for all matrices and are always real and non-negative.

## Fundamental Subspaces

The SVD illuminates the **four fundamental subspaces** of $A$:

| Subspace | Basis | Dimension |
|----------|-------|-----------|
| Column space $\text{Col}(A)$ | $u_1, \ldots, u_r$ | $r$ |
| Row space $\text{Row}(A)$ | $v_1, \ldots, v_r$ | $r$ |
| Null space $\text{Null}(A)$ | $v_{r+1}, \ldots, v_n$ | $n - r$ |
| Left null space $\text{Null}(A^T)$ | $u_{r+1}, \ldots, u_m$ | $m - r$ |

## The Eckart–Young Theorem

**Theorem (Eckart–Young–Mirsky).** The best rank-$k$ approximation to $A$ (in the Frobenius or operator norm) is:

$$
A_k = \sum_{i=1}^k \sigma_i \mathbf{u}_i \mathbf{v}_i^T
$$

The approximation error is:

$$
\|A - A_k\|_2 = \sigma_{k+1}, \quad \|A - A_k\|_F = \sqrt{\sigma_{k+1}^2 + \cdots + \sigma_r^2}
$$

This is the theoretical foundation for **dimensionality reduction**: retaining only the top $k$ singular values/vectors captures the most important features of $A$.

## Applications

### Principal Component Analysis (PCA)

Given a data matrix $X$ (centered), PCA computes the SVD $X = U\Sigma V^T$. The columns of $V$ (right singular vectors) are the **principal components** — the directions of maximum variance.

### The Pseudoinverse

The **Moore–Penrose pseudoinverse** of $A$ is:

$$
A^+ = V \Sigma^+ U^T, \quad \text{where } \Sigma^+ = \text{diag}(1/\sigma_1, \ldots, 1/\sigma_r, 0, \ldots, 0)
$$

It gives the **minimum-norm least-squares solution** to $Ax = b$:

$$
x^+ = A^+ b = \arg\min_x \|Ax - b\| \quad (\text{and the smallest } \|x\| \text{ among minimizers})
$$

### Image Compression

An image represented as an $m \times n$ matrix can be compressed by keeping only the top $k$ singular values. Storage drops from $mn$ to $k(m + n + 1)$ — dramatic savings when $k \ll \min(m,n)$.

### Numerical Rank and Conditioning

The **condition number** $\kappa(A) = \sigma_1/\sigma_r$ measures sensitivity to perturbations. Large $\kappa$ means $A$ is **ill-conditioned** — small errors in data cause large errors in solutions.

The **numerical rank** is the number of singular values above a threshold, accounting for floating-point errors.

## Computation

The SVD is computed iteratively via:
1. **Golub–Kahan bidiagonalization** (reduction to bidiagonal form via Householder reflections)
2. **Implicit QR iteration** on the bidiagonal matrix

The cost is $O(mn^2)$ for an $m \times n$ matrix with $m \geq n$ — though randomized algorithms achieve approximate SVDs much faster.

## Key Takeaways

1. The **SVD** exists for every matrix and decomposes it as rotation-scaling-rotation.
2. The **Eckart–Young theorem** makes the SVD optimal for low-rank approximation.
3. The SVD reveals the **four fundamental subspaces** and the **condition number**.
4. Applications span **PCA**, image compression, the pseudoinverse, and numerical stability.

## Further Reading

- *Matrix Analysis and Applied Linear Algebra* by Carl D. Meyer — excellent SVD treatment
- *Numerical Linear Algebra* by Trefethen and Bau — the computational perspective
- *Introduction to Linear Algebra* by Gilbert Strang — the four subspaces beautifully explained

## Related Posts

- [Linear Algebra: Eigenvalues, Eigenvectors, and the Spectral Theorem](/blog/eigenvalues-spectral-theorem)
- [Hilbert Spaces: The Geometry of Infinite Dimensions](/blog/hilbert-spaces-geometry)
- [Representation Theory: How Groups Act on Vector Spaces](/blog/representation-theory-groups)

---

*Every matrix tells a story through its singular values. More linear algebra on the [blog](/blog).*
